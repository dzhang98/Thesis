{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from fin_data.ipynb\n",
      "importing Jupyter notebook from data.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import tensorflow as tf\n",
    "from fin_data import data_generator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.merge import Add\n",
    "from keras.optimizers import Adam,Nadam\n",
    "from models import discriminator_model, generator_model_cnn,generator_model_mlp,generator_model_mlp_cnn,generator_model_mlp_cnn_plus\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "#import visualize\n",
    "#import stylized_facts as sf\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "#from shutil import copyfile\n",
    "#import argparse \n",
    "\n",
    "# parser = argparse.ArgumentParser(description='FIN-GAN implementation')\n",
    "# parser.add_argument('--batch_size',type=int,default=24)\n",
    "# parser.add_argument('--generator_model',type=str,default='mlp-cnn')\n",
    "# parser.add_argument('--epochs',type=int,default=10)\n",
    "# parser.add_argument('--batches',type=int,default=1024)\n",
    "# parser.add_argument('--folder_name',type=str,default='')\n",
    "# parser.add_argument('--generator_lr',type=float,default='2e-4')\n",
    "# parser.add_argument('--discriminator_lr',type=float,default='1e-5')\n",
    "# parser.add_argument('--log_interval',type=int,default=50)\n",
    "# parser.add_argument('--seed',type=int,default=1)\n",
    "\n",
    "#args = parser.parse_args()\n",
    "#seed(args.seed)\n",
    "#set_random_seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = data_generator()\n",
    "batch_size = 24\n",
    "dg.batch_size = batch_size\n",
    "batches = 1024\n",
    "epochs = 10                 \n",
    "timestamp = dt.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "timestamp += '_'\n",
    "#timestamp += args.folder_name\n",
    "# os.mkdir('./imgs/%s'%(timestamp))\n",
    "# os.mkdir('./npy/%s'%(timestamp))\n",
    "# os.mkdir('./weights/%s'%(timestamp))\n",
    "# os.mkdir('./imgs/%s/acf'%(timestamp))\n",
    "# os.mkdir('./imgs/%s/dist'%(timestamp))\n",
    "# os.mkdir('./imgs/%s/time_series'%(timestamp))\n",
    "# os.mkdir('./imgs/%s/leverage'%(timestamp))\n",
    "# copyfile('./main.py','./imgs/%s/main.py'%(timestamp))\n",
    "# copyfile('./models.py','./imgs/%s/models.py'%(timestamp))\n",
    "# with open('./imgs/%s/hyper_parameters.txt'%(timestamp),'w') as w:\n",
    "#     w.write(str(args))\n",
    "def train():\n",
    "    generator,generator_statistics = generator_model_mlp_cnn()\n",
    "    #preparing generator\n",
    "#     if args.generator_model == 'mlp-cnn':\n",
    "#         generator,generator_statistics = generator_model_mlp_cnn()\n",
    "#     elif args.generator_model == 'mlp':\n",
    "#         generator,generator_statistics = generator_model_mlp()\n",
    "#     elif args.generator_model == 'cnn':\n",
    "#         generator = generator_model_cnn()\n",
    "#     elif args.generator_model == 'plus':\n",
    "#         generator = generator_model_mlp_cnn_plus()\n",
    "#     else:\n",
    "#         import sys\n",
    "#         sys.exit()\n",
    "    #preparing discriminator\n",
    "                                                      \n",
    "    statistics_opt = Adam(lr=0.0001)\n",
    "    generator_statistics.compile(loss='mean_squared_error',optimizer=statistics_opt)\n",
    "\n",
    "    discriminator = discriminator_model()\n",
    "    d_opt = Adam(lr=1e-5, beta_1=0.1)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_opt)\n",
    "    discriminator.trainable = False\n",
    "    for e in discriminator.layers:\n",
    "        e.trainable = False\n",
    "    gan = Sequential([generator,discriminator])\n",
    "    g_opt = Adam(lr=2e-4, beta_1=0.5)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=g_opt)\n",
    "    \n",
    "    g_loss_recorder = []\n",
    "    d_loss_recorder = []\n",
    "    g_losses_recorder = []\n",
    "    d_losses_recorder = []\n",
    "    #start training\n",
    "    for epoch in range(epochs):\n",
    "        for index in range(batches):\n",
    "            noise = np.array([np.random.normal(0,1.0,size=100) for _ in range(batch_size)])\n",
    "            real_series = dg.index_data_random_picker()\n",
    "            real_series = np.nan_to_num(real_series)\n",
    "            generated_series = generator.predict(noise, verbose=0)\n",
    "#             if index % args.log_interval == 0:\n",
    "#                 sf.acf(generated_series,'./imgs/%s/acf/acf_abs_%i_%i'%(timestamp,epoch,index),for_abs=True)\n",
    "#                 sf.acf(generated_series,'./imgs/%s/acf/acf_raw_%i_%i'%(timestamp,epoch,index),for_abs=False)\n",
    "#                 sf.acf(generated_series,'./imgs/%s/acf/acf_abs_linear_%i_%i'%(timestamp,epoch,index),for_abs=True,scale='linear')\n",
    "#                 sf.acf(generated_series,'./imgs/%s/acf/acf_raw_linear_%i_%i'%(timestamp,epoch,index),for_abs=False,scale='linear')\n",
    "#                 sf.leverage_effect(generated_series,'./imgs/%s/leverage/leverage_%i_%i'%(timestamp,epoch,index))\n",
    "#                 sf.distribution(generated_series, './imgs/%s/dist/distribution_%i_%i'%(timestamp,epoch,index),'linear')\n",
    "#                 sf.distribution(generated_series, './imgs/%s/dist/distribution_%i_%i'%(timestamp,epoch,index),'log')\n",
    "#                 visualize.time_series(generated_series[0],'./imgs/%s/time_series/generated_time_series_%i_%i'%(timestamp,epoch,index))\n",
    "#                 np.save('./npy/%s/generated_time_series_%i_%i.npy'%(timestamp,epoch,index),generated_series)\n",
    "            # update discriminator\n",
    "            X = np.concatenate((real_series, generated_series))\n",
    "            y = np.concatenate([np.random.uniform(0.9,1.1,batch_size),np.random.uniform(0.1,0.3,batch_size)])\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            d_loss_recorder.append(d_loss)\n",
    "            # update generator\n",
    "            y = np.array([1.]*batch_size,dtype=np.float32)\n",
    "            g_loss = gan.train_on_batch(noise, y)\n",
    "            g_loss_recorder.append(g_loss)\n",
    "            print(\"epoch: %d, batch: %d, g_loss: %f, d_loss: %f\" % (epoch, index, g_loss, d_loss))\n",
    "            #generator.save_weights('./weights/%s/generator_%i_%i.h5'%(timestamp,epoch,index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
